{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # Use Selenium to visit the url, capture html\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    return Browser('chrome', **executable_path, headless=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_Mars():  \n",
    "    ###     ###     ###     inside or outside function??\n",
    "    browser = init_browser()     \n",
    "    # Create an empty dict to hold results\n",
    "    mars_results_dict = {}       \n",
    "    print(\"Start executing scrape_mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'browser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-39f1f26b1e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### (1) Retrieve most recent news article from NASA site\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murlNews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://mars.nasa.gov/news/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlNews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#retrieve HTML object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhtmlNews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'browser' is not defined"
     ]
    }
   ],
   "source": [
    "    #### (1) Retrieve most recent news article from NASA site\n",
    "    urlNews = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(urlNews)\n",
    "    #retrieve HTML object\n",
    "    htmlNews = browser.html\n",
    "    # parse HTML with Beautiful Soup, to find tags for desired content\n",
    "    soupNews = BeautifulSoup(htmlNews, 'html.parser')\n",
    "    # Use the closest tags to capture news articles, tagged as list items\n",
    "    resultNews = soupNews.find_all('li', class_=\"slide\")\n",
    "    # Capture the first instance of it, the most recent news article \n",
    "    # Use a more specific tag to capture just the text desired\n",
    "    news_title = soupNews.find('h3').text\n",
    "    news_body = soupNews.find('div', class_=\"rollover_description_inner\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Add collected items for webpage to a dictionary immediately...\n",
    "    # ...so as not to have to search up through code to find them later.\n",
    "    mars_results_dict[\"Mars_news_title\"] = news_title\n",
    "    mars_results_dict[\"Mars_news_body\"] = news_body\n",
    "\n",
    "    print(\"after News, before Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #### (2) Retrieve a featured image from NASA site, large version of file\n",
    "    # This page takes awhile to load... sleep 5seconds before parsing.\n",
    "    urlImg = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(urlImg)\n",
    "    time.sleep(2)\n",
    "    htmlImg = browser.html\n",
    "    soupImg = BeautifulSoup(htmlImg, 'html.parser')\n",
    "    # Within div:class=\"carousel_container\" is a footer button link to a smaller image \n",
    "    # But the larger image we want is the wallpaper in the \"article\" tag.\n",
    "    main_feature = soupImg.find('article', class_='carousel_item')       \n",
    "    main_alt = soupImg.find('a', class_='button fancybox')\n",
    "    image_suffix = main_alt['data-fancybox-href']\n",
    "    # This is not a complete URL; give it a prefix so it can be used directly as URL.\n",
    "    image_prefix = \"https://www.jpl.nasa.gov\"\n",
    "    marsImg_link = image_prefix + image_suffix\n",
    "\n",
    "    mars_results_dict[\"MarsImg_link\"] = marsImg_link\n",
    "\n",
    "    print(\"after Image, before Weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #### (3) Mars Weather tweet: scrape the latest Mars weather report as `mars_weather`.\n",
    "    urlWea = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    ## Question: .get is as good as \".post\" for the entire assignment?? because we are\n",
    "    ## passing only url (no params) and not re-submitting request...safe but unnecessary.\n",
    "    responseWea = requests.get(urlWea)\n",
    "    soupWea = BeautifulSoup(responseWea.text, 'html.parser')\n",
    "    resultsWea = soupWea.find_all('div', class_=\"js-tweet-text-container\")\n",
    "    mars_weather = resultsWea[0].find('p',{'class':'js-tweet-text'}).text\n",
    "\n",
    "    mars_results_dict[\"Mars_weather\"] = mars_weather\n",
    "\n",
    "    print(\"after Weather, before StatsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #### (4) From NASA facts page on mars, retrieve the table of vital statistics.\n",
    "    # Visit (http://space-facts.com/mars/) & panda-scrape the Mars table of facts;\n",
    "    # Use Pandas to convert the data to a HTML table string\n",
    "    urlStats = 'https://space-facts.com/mars/'\n",
    "    tables = pd.read_html(urlStats)\n",
    "    # Convert that html table to a dataframe and clean it up.\n",
    "    marsStats_df = tables[0]\n",
    "    marsStats_df.columns = ['Description', 'Value']\n",
    "    marsStats_df.set_index('Description', inplace=True)\n",
    "    # Mars_df.head()\n",
    "    # Write this table(dataframe) to an HTML string. Clean it up a bit.\n",
    "    stats_html_string = marsStats_df.to_html().replace('\\n', '')\n",
    "    # # Write the string to an HTML file; can open in browser to verify.\n",
    "    # stats_html.to_html('MarsStatsTable.html')\n",
    "    # !open MarsTable.html\n",
    "\n",
    "    mars_results_dict[\"Stats_html_string\"] = stats_html_string\n",
    "\n",
    "    print(\"after StatsTable, before Hemis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #### (5) Get 4 images of Mars' Hemispheres\n",
    "    # Visit (https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars)\n",
    "    # Obtain images for each of Mar's hemispheres (click thru links to get full-res images).\n",
    "    # For each: Save url string and title w/hemisphere name to a  Python dictionary \n",
    "    # using keys `img_url` and `title`; 1 dictionary for each hemisphere. Append each to a list.\n",
    "    # Use splinter and soup to retrieve html and convert to soup object. \n",
    " \n",
    "    urlHemis = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    browser.visit(urlHemis)\n",
    "    # Pages take a few seconds to load; wait, and generate result <after> page is loaded\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    html = browser.html\n",
    "    soupHemi = BeautifulSoup(html, 'html.parser')\n",
    "    # Retrieve all elements (in a glob) that contain hemisphere content/links\n",
    "    hemis_div = soupHemi.find_all('div', class_='item')\n",
    "    # Create empty list to which will append 1 dictionary per hemisphere. Save URL prefix.\n",
    "    hemispheres = []\n",
    "    url_prefix = \"https://astrogeology.usgs.gov\"\n",
    "    # Iterate through \"item\"s to capture target urls containing the hemisphere images\n",
    "    for hemi in hemis_div:\n",
    "        # Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "        hemi_title.append(hemis_div.h3.text.strip())\n",
    "        partial_url = hemis_div.find('a')['href']\n",
    "        full_url = url_prefix + partial_url\n",
    "        # Create a dictionary of the title and url. \n",
    "        hemi_dict = {}\n",
    "        hemi_dict.title = hemi_title\n",
    "        hemi_dict.img_url = full_url\n",
    "        # hemi_dict['title'] = hemi_title\n",
    "        # hemi_dict['img_url'] = full_url\n",
    "        ### Append to the hemispheres list.\n",
    "        hemispheres.append(hemi_dict)\n",
    "\n",
    "    mars_results_dict[\"Hemispheres\"] = hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # I found analagous central locations on earth -> Hemispheres more comprehensible ?   \n",
    "    # center_landmark = [\"Hawaii\", \"Vietnam\", \"BikiniAtoll\", \"SriLanka\"]\n",
    "    # Hemispheres = dict(zip(center_landmark, zip(title, img_url)))\n",
    "\n",
    "    print(\"after hemispheres, before date-time\")\n",
    "\n",
    "    #### (5) Return the dictionary that collects the results of each section...\n",
    "    #     [\"Mars_news_title\"], [\"Mars_news_body\"], [\"MarsImg_link\"],[\"Mars_weather\"],\n",
    "    #     [\"Stats_html_string\"],[\"Hemispheres\"], [\"Date_time\"]\n",
    "    ### ...and capture the date, add to the dictionary.\n",
    "    cur_datetime = datetime.datetime.utcnow()\n",
    "    mars_results_dict[\"Date_time\"] = cur_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    return mars_results_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    browser.quit()    \n",
    "\n",
    "print(\"after browserQuit\")\n",
    "# result_A = scrape_Mars()\n",
    "# print(result_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
